{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FB30f4wYwSvg"
   },
   "source": [
    "## Dataset and loaders for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raEOHrpnbbKY"
   },
   "source": [
    "Keeping the same order, concatenate all the cleaned words from each caption into a string again, and add them all to a list of strings ```cleaned_captions```. Store all the image ids in a list ```image_ids```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGGnaDIRbZUs"
   },
   "outputs": [],
   "source": [
    "#if include all repeated image_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_FbII1VwVSg"
   },
   "source": [
    "The dataframe for the image paths and captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYQz4T3mwA2o"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'image_id': image_ids,\n",
    "    'path': [image_dir + image_id + \".jpg\" for image_id in image_ids],\n",
    "    'caption': cleaned_captions\n",
    "}\n",
    "\n",
    "data_df = pd.DataFrame(data, columns=['image_id', 'path', 'caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POB7UiJLwYsf"
   },
   "outputs": [],
   "source": [
    "data_df['caption'][38429]\n",
    "data_df_test=data_df[38430:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Wb_oFvwwFIJn",
    "outputId": "8aab6086-7a8f-4ef1-e2d5-6bd194f02c58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a brown and white sheltie leaping over a rail'"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_test['caption'][38430+5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNLQ0K-_weJy"
   },
   "source": [
    "This is the Flickr8k class for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxnyf3WnvVrS"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "from nltk import tokenize\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class Flickr8k(Dataset):\n",
    "    \"\"\" Flickr8k custom dataset compatible with torch.utils.data.DataLoader. \"\"\"\n",
    "    \n",
    "    def __init__(self, df, vocab, transform=None):\n",
    "        \"\"\" Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            df: df containing image paths and captions.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Returns one data pair (image and caption). \"\"\"\n",
    "\n",
    "        vocab = self.vocab\n",
    "\n",
    "        caption = self.df['caption'][index]\n",
    "        img_id = self.df['image_id'][index]\n",
    "        path = self.df['path'][index]\n",
    "\n",
    "        image = Image.open(open(path, 'rb'))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        tokens = caption.split()\n",
    "        caption = []\n",
    "        # Build the Tensor version of the caption, with token words\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vkld_4CwkPO"
   },
   "source": [
    "We need to overwrite the default PyTorch ```collate_fn()``` because our ground truth captions are sequential data of varying lengths. The default ```collate_fn()``` does not support merging the captions with padding.\n",
    "\n",
    "You can read more about it here: https://pytorch.org/docs/stable/data.html#dataloader-collate-fn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5YmKr9ewkqO"
   },
   "outputs": [],
   "source": [
    "def caption_collate_fn(data):\n",
    "    \"\"\" Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length from longest to shortest.\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6VDx2O5FSiM"
   },
   "source": [
    "Now we define the data transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpRbVk6BFTGD"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Crop size matches the input dimensions expected by the pre-trained ResNet\n",
    "data_transform = transforms.Compose([ \n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),  # Why do we choose 224 x 224?\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),   # Using ImageNet norms\n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgS9OpZ7FaAj"
   },
   "source": [
    "Initialising the datasets. The only twist is that every image has 5 ground truth captions, so each image appears five times in the dataframe. We don't want an image to appear in more than one set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnTvR684GGVV"
   },
   "outputs": [],
   "source": [
    "unit_size = 5\n",
    "\n",
    "train_split = 0.95 # Defines the ratio of train/test data.\n",
    "\n",
    "# We didn't shuffle the dataframe yet so this works\n",
    "train_size = unit_size * round(len(data_df)*train_split / unit_size)\n",
    "\n",
    "dataset_train = Flickr8k(\n",
    "    df=data_df[:train_size].reset_index(drop=True),\n",
    "    vocab=vocab,\n",
    "    transform=data_transform,\n",
    ")\n",
    "\n",
    "dataset_test = Flickr8k(\n",
    "    df=data_df[(train_size):].reset_index(drop=True),\n",
    "    vocab=vocab,\n",
    "    transform=data_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfyCI0Wjhcy7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWuWg72dGOq9"
   },
   "source": [
    "Write the dataloaders ```train_loader``` and ```test_loader``` - explicitly replacing the collate_fn:\n",
    "\n",
    "```train_loader = torch.utils.data.DataLoader(\n",
    "  ...,\n",
    "  collate_fn=caption_collate_fn\n",
    ")```\n",
    "\n",
    "Set train batch size to 128 and be sure to set ```shuffle=True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkNrIRbXGLFG"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data \n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, \n",
    "                                                batch_size=128,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=0,\n",
    "                                                collate_fn=caption_collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, \n",
    "                                                batch_size=5,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=0,\n",
    "                                                collate_fn=caption_collate_fn)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COMP5623_CW2_Starter_08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
