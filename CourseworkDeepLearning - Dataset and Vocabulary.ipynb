{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81kdnnwJvTFx"
   },
   "source": [
    "## Text preparation \n",
    "\n",
    "build a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "collapsed": true,
    "id": "y9DYUTXNRJFr",
    "outputId": "2db1920a-f41c-4cd2-86fd-912918951d87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXpWOFqFOXcc"
   },
   "outputs": [],
   "source": [
    "# Mounted Drive if using Colab; otherwise, your local path\n",
    "root = \"/drive/My Drive/Colab Notebooks/Flickr8k/\" # <--- replace this with your root data directory\n",
    "#/drive/My Drive/Colab Notebooks/Flickr8k/Flickr8k_Dataset\n",
    "caption_dir = root + \"Flickr8k_text/\"                      # <--- replace these too\n",
    "image_dir = root + \"Flickr8k_Dataset/\"                            # <---\n",
    "#/drive/My Drive/Colab Notebooks/Flickr8k/Flickr8k_text/Flickr8k.token.txt\n",
    "\n",
    "token_file = \"Flickr8k.token.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9AkORttFoF_"
   },
   "source": [
    "A helper function to read in our ground truth text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHC0y7zaOXq8"
   },
   "outputs": [],
   "source": [
    "def read_lines(filepath):\n",
    "    \"\"\" Open the ground truth captions into memory, line by line. \"\"\"\n",
    "    file = open(filepath, 'r')\n",
    "    lines = []\n",
    "\n",
    "    while True: \n",
    "        # Get next line from file until there's no more\n",
    "        line = file.readline() \n",
    "        if not line: \n",
    "            break\n",
    "        lines.append(line.strip())\n",
    "    file.close() \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D86cJx2yv81K"
   },
   "source": [
    "read all the ground truth captions (5 per image), into memory as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9m-snsM2XHuu"
   },
   "outputs": [],
   "source": [
    "lines = read_lines(caption_dir + token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-IkK91ZuXNB2",
    "outputId": "c289b94d-a9cd-49d2-bdee-1588bde081ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40455"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNp6sT8gCSEd"
   },
   "outputs": [],
   "source": [
    "file = open(caption_dir + token_file, 'r') #remove \n",
    "doc = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOEo4_k0FIEj"
   },
   "outputs": [],
   "source": [
    "def load_captions(doc):\n",
    "    mapping = dict()\n",
    "# process lines\n",
    "    for line in doc.split('\\n'):\n",
    "# split line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "# take the first token as the image id, the rest as the description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "# extract filename from image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "# convert description tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "# create the list if needed\n",
    "        if image_id not in mapping:\n",
    "           mapping[image_id] = list()\n",
    "# store description\n",
    "           mapping[image_id].append(image_desc)\n",
    "    return mapping\n",
    "\n",
    "# parse descriptions\n",
    "captions_dict = load_captions(doc)\n",
    "#print('Loaded: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "2TXzgY4wYcMv",
    "outputId": "11f42e4f-5c41-4482-cf55-d2d2361dee2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization?\n",
    "Lemmatization is not necessary. We want to compare performance on different decoder normal RNN and LSTM.  It is probably better to control other factors that may affect performance of these two models if we want a better comparison. The main reason of using lemmatization is increasing accuracy of generating caption, so if we want to develop a better model to obtain more accurate caption, we should consider using lemmatized token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-K1csvyZk3f"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "counter=Counter()\n",
    "tokens=[]\n",
    "filtered_captions_dict = {}\n",
    "#ids=captions_dict.keys\n",
    "for image, captions in captions_dict.items():\n",
    "    filtered_captions_dict[image] = []\n",
    "    for caption in captions:\n",
    "        caption = caption.translate(str.maketrans('', '', string.punctuation))\n",
    "        caption=str(caption)\n",
    "        caption=caption.lower()\n",
    "        tokens.extend(nltk.tokenize.word_tokenize(caption))\n",
    "        filtered_captions_dict[image].append(caption.split())\n",
    "  \n",
    "    counter.update(tokens)\n",
    "\n",
    "\n",
    "# If the word frequency is less than 'threshold', then the word is discarded.\n",
    "#words= [word for word in words if word.isalnum()]\n",
    "words = [word for word, cnt in counter.items() if cnt > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAA7xHuj5j_P"
   },
   "outputs": [],
   "source": [
    "unique_filtered_captions_dict= {}\n",
    "cleaned_captions = []\n",
    "image_ids = [] \n",
    "for key, values in filtered_captions_dict.items():\n",
    "    for caption in values:\n",
    "        cleaned_caption_joined = None\n",
    "        image_ids.append(key)\n",
    "    for word in caption:\n",
    "        if word in words:\n",
    "        cleaned_caption_joined = cleaned_caption_joined + \" \" + word if cleaned_caption_joined else word\n",
    "    cleaned_captions.append(cleaned_caption_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnjFJQUKXyY0"
   },
   "source": [
    "captions_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oksUJjLPwApA"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
    "    def __init__(self):\n",
    "        # Intially, set both the IDs and words to empty dictionaries.\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # If the word does not already exist in the dictionary, add it\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            # Increment the ID for the next word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        # If we try to access a word in the dictionary which does not exist, return the <unk> id\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEQtthpXwEoY"
   },
   "source": [
    "Extract all the words from ```lines```, and create a list of them in a variable ```words```, for example:\n",
    "\n",
    "```words = [\"a\", \"an\", \"the\", \"cat\"... ]```\n",
    "\n",
    "No need to worry about duplicates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHBMe-ATwLIQ"
   },
   "source": [
    "Build the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctwErx_ZwAzB"
   },
   "outputs": [],
   "source": [
    "# Create a vocab instance\n",
    "vocab = Vocabulary()\n",
    "\n",
    "# Add the token words first\n",
    "vocab.add_word('<pad>')\n",
    "vocab.add_word('<start>')\n",
    "vocab.add_word('<end>')\n",
    "vocab.add_word('<unk>')\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    vocab.add_word(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzEYIvJ-GA_G"
   },
   "source": [
    "Add the rest of the words from the parsed captions:\n",
    "\n",
    "``` vocab.add_word('new_word')```\n",
    "\n",
    "Don't add words that appear three times or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Zj99JT2XwA4-",
    "outputId": "98e9c246-25fd-4974-f60c-3a49df2cd858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 3440\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "vocab_path=F'/drive/My Drive/Colab Notebooks/Flickr8k/vocab.pkl'\n",
    "vocab_path = vocab_path\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Total vocabulary size: {}\".format(len(vocab)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COMP5623_CW2_Starter_08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}